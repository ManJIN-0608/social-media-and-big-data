# Load packages
pkgs <- c("rtweet","ggplot2", "tm", 'Rspotify', "spotifyr", "dplyr",
"plyr", "caret")
lapply(pkgs, library, character.only=TRUE)
# Use twitter API to get the followers of Griffith Uni and information about each.
CP_followers <- get_followers("Charlie Puth", n = 10000)
user_data.df <- lookup_users(CP_followers$user_id)
View(user_data.df)
# Use twitter API to get the followers of Griffith Uni and information about each.
CP_followers <- get_followers("charlieputh", n = 10000)
user_data.df <- lookup_users(CP_followers$user_id)
View(user_data.df)
# Plot scatter of Friends Count vs Followers Count.
ggplot(data=user_data.df, aes(x=followers_count, y=friends_count)) +
geom_point(stat = "identity") +
xlab("Followers Count") +
ylab("Friends Count") +
ggtitle("Friends vs Followers")
# Plot scatter of Friends Count vs Followers Count (using logarithmic scales).
ggplot(data=user_data.df, aes(x=log(followers_count), y=log(friends_count))) +
geom_point(stat = "identity") +
xlab("Log Followers Count") +
ylab("Log Friends Count") +
ggtitle("Friends vs Followers - Log 10 Scale")
# Get Friends Count and Followers Count under logarithmic functions.
logFriendsCount <- log(user_data.df$friends_count)
logFollowersCount <- log(user_data.df$followers_count)
# Create data frame with log data derived above
kObject.log <- data.frame(user_data.df$name, logFollowersCount, logFriendsCount)
View(kObject.log)
# Remove invalid records from the data frame.
kObject.log <- subset(kObject.log, kObject.log$logFriendsCount!="-Inf")
kObject.log <- subset(kObject.log, kObject.log$logFollowersCount!="-Inf")
View(kObject.log)
# Load packages
pkgs <- c("rtweet","ggplot2", "tm", 'Rspotify', "spotifyr", "dplyr",
"plyr", "caret")
lapply(pkgs, library, character.only=TRUE)
# Use twitter API to get the followers of Griffith Uni and information about each.
CP_followers <- get_followers("DerikFein", n = 1000)
user_data.df <- lookup_users(CP_followers$user_id)
# Use twitter API to get the followers of Griffith Uni and information about each.
DF_followers <- get_followers("DerikFein", n = 1000)
user_data.df <- lookup_users(DF_followers$user_id)
View(user_data.df)
# Plot scatter of Friends Count vs Followers Count.
ggplot(data=user_data.df, aes(x=followers_count, y=friends_count)) +
geom_point(stat = "identity") +
xlab("Followers Count") +
ylab("Friends Count") +
ggtitle("Friends vs Followers")
# Plot scatter of Friends Count vs Followers Count (using logarithmic scales).
ggplot(data=user_data.df, aes(x=log(followers_count), y=log(friends_count))) +
geom_point(stat = "identity") +
xlab("Log Followers Count") +
ylab("Log Friends Count") +
ggtitle("Friends vs Followers - Log 10 Scale")
# Get Friends Count and Followers Count under logarithmic functions.
logFriendsCount <- log(user_data.df$friends_count)
logFollowersCount <- log(user_data.df$followers_count)
# Create data frame with log data derived above
kObject.log <- data.frame(user_data.df$name, logFollowersCount, logFriendsCount)
View(kObject.log)
# Remove invalid records from the data frame.
kObject.log <- subset(kObject.log, kObject.log$logFriendsCount!="-Inf")
kObject.log <- subset(kObject.log, kObject.log$logFollowersCount!="-Inf")
# Run KMeans Clustering Algorithm.
user2Means.log <- kmeans(kObject.log[,2:ncol(kObject.log)], centers=5, iter.max=10, nstart=10)
kObject.log$cluster=factor(user2Means.log$cluster)
View(kObject.log)
# Load packages
pkgs <- c("rtweet","ggplot2", "tm", 'Rspotify', "spotifyr", "dplyr",
"plyr", "caret")
lapply(pkgs, library, character.only=TRUE)
# Use twitter API to get the followers of Charlie Puth and information about each.
CP_followers <- get_followers("charlieputh", n = 1000)
user_data.df <- lookup_users(CP_followers$user_id)
View(user_data.df)
# Plot scatter of Friends Count vs Followers Count.
ggplot(data=user_data.df, aes(x=followers_count, y=friends_count)) +
geom_point(stat = "identity") +
xlab("Followers Count") +
ylab("Friends Count") +
ggtitle("Friends vs Followers")
# Plot scatter of Friends Count vs Followers Count (using logarithmic scales).
ggplot(data=user_data.df, aes(x=log(followers_count), y=log(friends_count))) +
geom_point(stat = "identity") +
xlab("Log Followers Count") +
ylab("Log Friends Count") +
ggtitle("Friends vs Followers - Log 10 Scale")
# Get Friends Count and Followers Count under logarithmic functions.
logFriendsCount <- log(user_data.df$friends_count)
logFollowersCount <- log(user_data.df$followers_count)
# Create data frame with log data derived above
kObject.log <- data.frame(user_data.df$name, logFollowersCount, logFriendsCount)
View(kObject.log)
# Remove invalid records from the data frame.
kObject.log <- subset(kObject.log, kObject.log$logFriendsCount!="-Inf")
kObject.log <- subset(kObject.log, kObject.log$logFollowersCount!="-Inf")
# Run KMeans Clustering Algorithm.
user2Means.log <- kmeans(kObject.log[,2:ncol(kObject.log)], centers=5, iter.max=10, nstart=10)
kObject.log$cluster=factor(user2Means.log$cluster)
View(kObject.log)
# Load packages
pkgs <- c("rtweet","ggplot2", "tm", 'Rspotify', "spotifyr", "dplyr",
"plyr", "caret")
lapply(pkgs, library, character.only=TRUE)
# Use twitter API to get the followers of Charlie Puth and information about each.
CP_followers <- get_followers("charlieputh", n = 10000)
user_data.df <- lookup_users(CP_followers$user_id)
View(user_data.df)
# Plot scatter of Friends Count vs Followers Count.
ggplot(data=user_data.df, aes(x=followers_count, y=friends_count)) +
geom_point(stat = "identity") +
xlab("Followers Count") +
ylab("Friends Count") +
ggtitle("Friends vs Followers")
# Plot scatter of Friends Count vs Followers Count (using logarithmic scales).
ggplot(data=user_data.df, aes(x=log(followers_count), y=log(friends_count))) +
geom_point(stat = "identity") +
xlab("Log Followers Count") +
ylab("Log Friends Count") +
ggtitle("Friends vs Followers - Log 10 Scale")
# Get Friends Count and Followers Count under logarithmic functions.
logFriendsCount <- log(user_data.df$friends_count)
logFollowersCount <- log(user_data.df$followers_count)
# Create data frame with log data derived above
kObject.log <- data.frame(user_data.df$name, logFollowersCount, logFriendsCount)
View(kObject.log)
# Remove invalid records from the data frame.
kObject.log <- subset(kObject.log, kObject.log$logFriendsCount!="-Inf")
kObject.log <- subset(kObject.log, kObject.log$logFollowersCount!="-Inf")
# Run KMeans Clustering Algorithm.
user2Means.log <- kmeans(kObject.log[,2:ncol(kObject.log)], centers=5, iter.max=10, nstart=10)
kObject.log$cluster=factor(user2Means.log$cluster)
View(kObject.log)
# Plot cluster data.
ggplot(data=kObject.log,
aes(x=logFollowersCount, y=logFriendsCount, colour=cluster)) +
geom_point(stat = "identity") +
xlab("Log Followers Count") +
ylab("Log Friends Count") +
ggtitle("Friends vs Followers - Log 10 Scale")
user2Means.log$centers
# Use twitter API to get the followers of Derik Fein and information about each.
DF_followers <- get_followers("DerikFein", n = 10000)
user_data.df <- lookup_users(DF_followers$user_id)
View(user_data.df)
# Plot scatter of Friends Count vs Followers Count.
ggplot(data=user_data.df, aes(x=followers_count, y=friends_count)) +
geom_point(stat = "identity") +
xlab("Followers Count") +
ylab("Friends Count") +
ggtitle("Friends vs Followers")
# Plot scatter of Friends Count vs Followers Count (using logarithmic scales).
ggplot(data=user_data.df, aes(x=log(followers_count), y=log(friends_count))) +
geom_point(stat = "identity") +
xlab("Log Followers Count") +
ylab("Log Friends Count") +
ggtitle("Friends vs Followers - Log 10 Scale")
# Get Friends Count and Followers Count under logarithmic functions.
logFriendsCount <- log(user_data.df$friends_count)
logFollowersCount <- log(user_data.df$followers_count)
# Create data frame with log data derived above
kObject.log <- data.frame(user_data.df$name, logFollowersCount, logFriendsCount)
View(kObject.log)
# Remove invalid records from the data frame.
kObject.log <- subset(kObject.log, kObject.log$logFriendsCount!="-Inf")
kObject.log <- subset(kObject.log, kObject.log$logFollowersCount!="-Inf")
# Run KMeans Clustering Algorithm.
user2Means.log <- kmeans(kObject.log[,2:ncol(kObject.log)], centers=5, iter.max=10, nstart=10)
kObject.log$cluster=factor(user2Means.log$cluster)
View(kObject.log)
# Plot cluster data.
ggplot(data=kObject.log,
aes(x=logFollowersCount, y=logFriendsCount, colour=cluster)) +
geom_point(stat = "identity") +
xlab("Log Followers Count") +
ylab("Log Friends Count") +
ggtitle("Friends vs Followers - Log 10 Scale")
app_id <- "8fee083e5ddc41948dc9836b9444c0fb"
app_secret <- "3c36b666d22349ceb33660ae50cf06a6"
token <- "1"
keys <- spotifyOAuth(token, app_id, app_secret)
View(charliesongs)
# Get Songs from Charlie Puth (and their audio features)
charliesongs <- get_artist_audio_features('Charlie Puth')
View(charliesongs)
app_id <- "8fee083e5ddc41948dc9836b9444c0fb"
app_secret <- "3c36b666d22349ceb33660ae50cf06a6"
token <- "1"
keys <- spotifyOAuth(token, app_id, app_secret)
Sys.setenv(SPOTIFY_CLIENT_ID = app_id)
Sys.setenv(SPOTIFY_CLIENT_SECRET = app_secret)
access_token <- get_spotify_access_token()
# Get Songs from Charlie Puth (and their audio features)
charliesongs <- get_artist_audio_features('Charlie Puth')
View(charliesongs)
# Get Drake Songs that have a full record associated with them
getScore <- subset(charliesongs, track_name!="NA")
median(getScore$valence)
mean(getScore$valence)
# Plot Drake Song Valence Data.
ggplot(data=getScore,
aes(x=getScore$valence, y=getScore$track_name)) +
geom_point() +
theme(axis.text.y = element_text(size = 5)) +
xlab("Valence Score") +
ylab("Album") +
ggtitle("Charlie Puth's Valence Score")
# Get songs and features from another artist (Enya) - same way done for Drake
dnceSongs = get_artist_audio_features('DNCE')
getScoreDnce <- subset(dnceSongs, track_name!="NA")
View(getScoreDnce)
# Add the isCharlie column to each score array (to indicate which songs are Charlie and which are not)
getScoreDnce["isCharlie"] <- 0
getScore["isCharlie"] <- 1
# Combine the score arrays from the two artists
dataSet <- rbind.fill(getScoreDnce, getScore)
dataSet <- dataSet[!duplicated(dataSet),]
View(getScoreDnce)
# Add the isCharlie column to each score array (to indicate which songs are Charlie and which are not)
getScoreDnce["isCharlie"] <- 0
getScore["isCharlie"] <- 1
# Combine the score arrays from the two artists
dataSet <- rbind.fill(getScoreDnce, getScore)
dataSet <- dataSet[!duplicated(dataSet),]
# Select only the feature columns and isCharlie column.
cols <- c(9:19, 40)
dataSet <- dataSet[,cols]
View(dataSet[1,])
# Change the isCharlie column into a factor (for both testing and training sets)
dataSet$isCharlie <- factor(dataSet$isCharlie)
# Randomize data set
dataSet <- dataSet[sample(1:nrow(dataSet)),]
# Split the data set into training and testing sets (80% training set, 20% testing)
splitPoint <- as.integer(nrow(dataSet)*0.8)
trainingSet <- dataSet[1:splitPoint,]
testingSet <- dataSet[(splitPoint+1):nrow(dataSet),]
# Train the CART Model
prediction.r <- train(isCharlie~ ., data=trainingSet, method="rpart")
prediction.r
# Sample a single prediction (can repeat)
prediction_row = 1 # MUST be smaller or equal to than training data set size
if (predict(prediction.r, testingSet[prediction_row,]) ==
testingSet[prediction_row, 12]){
print("Correct!")
} else{
("Wrong.")
}
# Analyse the model accuracy using the Confusion Matrix
confusionMatrix(prediction.r, reference = testingSet$isCharlie)
draw.tree(prediction.r)
plot(prediction.r)
albums = get_albumn_audio_features('album', '0t0NkQulrNkxw2oUZZHboA')
albums = get_album_audio_features('album', '0t0NkQulrNkxw2oUZZHboA')
albums = get_audio_features('album', '0t0NkQulrNkxw2oUZZHboA')
songs = getFeatures('0t0NkQulrNkxw2oUZZHboA')
songs = getFeatures('0t0NkQulrNkxw2oUZZHboA',token=keys)
View(songs)
songs<-getFeatures('0t0NkQulrNkxw2oUZZHboA',token=keys)
songs<-getFeatures('7E4Zm8sWQgkGyOm2v4d9D4',token=keys)
View(songs)
album.songs<-getAlbum('7E4Zm8sWQgkGyOm2v4d9D4',token=keys)
View(album.songs)
song <- getFeatures('36cqLXl2jfD4nMAfFKyuZ5', token = keys)
View(song)
View(getScoreDnce)
View(dataSet)
View(dnceSongs)
getScoreSong <- subset(song, track_name!="NA")
# Add the isCharlie column to each score array (to indicate which songs are Charlie and which are not)
song["isCharlie"] <- 0
getScore["isCharlie"] <- 1
# Combine the score arrays from the two artists
dataSet <- rbind.fill(song, getScore)
dataSet <- dataSet[!duplicated(dataSet),]
# Select only the feature columns and isCharlie column.
cols <- c(9:19, 40)
dataSet <- dataSet[,cols]
View(dataSet[1,])
# Change the isCharlie column into a factor (for both testing and training sets)
dataSet$isCharlie <- factor(dataSet$isCharlie)
# Randomize data set
dataSet <- dataSet[sample(1:nrow(dataSet)),]
# Split the data set into training and testing sets (80% training set, 20% testing)
splitPoint <- as.integer(nrow(dataSet)*0.8)
trainingSet <- dataSet[1:splitPoint,]
testingSet <- dataSet[(splitPoint+1):nrow(dataSet),]
# Train the CART Model
prediction.r <- train(isCharlie~ ., data=trainingSet, method="rpart")
prediction.r
# Get Songs from Charlie Puth (and their audio features)
charliesongs <- get_artist_audio_features('Charlie Puth')
app_id <- "8fee083e5ddc41948dc9836b9444c0fb"
app_secret <- "3c36b666d22349ceb33660ae50cf06a6"
token <- "1"
keys <- spotifyOAuth(token, app_id, app_secret)
# Load packages
pkgs <- c("rtweet","ggplot2", "tm", 'Rspotify', "spotifyr", "dplyr", "plyr", "caret")
lapply(pkgs, library, character.only=TRUE)
app_id <- "8fee083e5ddc41948dc9836b9444c0fb"
app_secret <- "3c36b666d22349ceb33660ae50cf06a6"
token <- "1"
keys <- spotifyOAuth(token, app_id, app_secret)
Sys.setenv(SPOTIFY_CLIENT_ID = app_id)
Sys.setenv(SPOTIFY_CLIENT_SECRET = app_secret)
access_token <- get_spotify_access_token()
# Get Songs from Charlie Puth (and their audio features)
charliesongs <- get_artist_audio_features('Charlie Puth')
View(charliesongs)
# Get Drake Songs that have a full record associated with them
getScore <- subset(charliesongs, track_name!="NA")
median(getScore$valence)
mean(getScore$valence)
# Plot Charlie Song Valence Data.
ggplot(data=getScore,
aes(x=getScore$valence, y=getScore$track_name)) +
geom_point() +
theme(axis.text.y = element_text(size = 5)) +
xlab("Valence Score") +
ylab("Album") +
ggtitle("Charlie Puth's Valence Score")
# Get songs and features from a playlist - same way done for Charlie
dncesongs <- get_artist_audio_features('DNCE')
getScoreDnce <- subset(dncesongs, track_name!="NA")
zaynsongs <- get_artist_audio_features('ZAYN')
getScoreZayn <- subset(zaynsongs, track_name!="NA")
njsongs <- get_artist_audio_features('Nick Jonas')
getScoreNj <- subset(njsongs, track_name!="NA")
# Add the isCharlie column to each score array (to indicate which songs are Charlie and which are not)
getScoreDnce["isCharlie"] <- 0
getScore["isCharlie"] <- 1
getScoreZayn["isCharlie"] <- 0
getScoreNj["isCharlie"] <- 0
# Combine the score arrays from the two artists
dataSet <- rbind.fill(getScoreDnce, getScore, getScoreZayn, getScoreNj)
dataSet <- dataSet[!duplicated(dataSet),]
View(dataSet)
# Select only the feature columns and isCharlie column.
cols <- c(9:19, 40)
dataSet <- dataSet[,cols]
View(dataSet[1,])
# Change the isCharlie column into a factor (for both testing and training sets)
dataSet$isCharlie <- factor(dataSet$isCharlie)
# Randomize data set
dataSet <- dataSet[sample(1:nrow(dataSet)),]
View(dataSet)
# Split the data set into training and testing sets (80% training set, 20% testing)
splitPoint <- as.integer(nrow(dataSet)*0.8)
trainingSet <- dataSet[1:splitPoint,]
testingSet <- dataSet[(splitPoint+1):nrow(dataSet),]
# Train the CART Model
prediction.r <- train(isCharlie~ ., data=trainingSet, method="rpart")
prediction.r
plot(prediction.r)
# Sample a single prediction (can repeat)
prediction_row = 1 # MUST be smaller or equal to than training data set size
if (predict(prediction.r, testingSet[prediction_row,]) ==
testingSet[prediction_row, 12]){
print("Correct!")
} else{
("Wrong.")
}
# Sample a single prediction (can repeat)
successRate = 0
successPercentage = 0
for (pRow in 1:nrow(testingSet)){
prediction_row = pRow # MUST be smaller or equal to than training data set size
if (predict(prediction.r, testingSet[prediction_row,]) ==
testingSet[prediction_row, 12]){
print("Correct!")
successRate = successRate + 1
} else{
("Wrong.")
}
}
# Sample a single prediction (can repeat)
successRate = 0
successPercentage = 0
for (pRow in 1:nrow(testingSet)){
print(paste("testing row:", pRow))
prediction_row = pRow # MUST be smaller or equal to than training data set size
if (predict(prediction.r, testingSet[prediction_row,]) ==
testingSet[prediction_row, 12]){
print("Correct!")
successRate = successRate + 1
} else{
("Wrong.")
}
}
successPercentage = floor(successRate / nrow(testingSet) * 100)
a = paste("Rate of Success Predictions:", successPercentage, "%")
print(a)
prediction.r
pkgs <- c('vosonSML', 'rtweet', 'stringr', 'tm', 'SnowballC', 'ggplot2', 'igraph',
'slam', 'Rmpfr', 'rjson', 'leaflet', 'leaflet', 'httpuv', 'devtools', 'dplyr')
lapply(pkgs, library, character.only = TRUE)
View(trends_available())
current_trends <- get_trends(23424748)
View(current_trends)
lat1 <- 47.608013
long1 <- -122.335167
lat2 <- 29.424349
long2 <- -98.491142
map <- leaflet() %>%
addTiles() %>%
fitBounds(long1, lat1, long2, lat2)
map
geo_tweet <- search_tweets("charlieputh", lang="en", n=1000)
geo_tweet_df <- lat_lng(geo_tweet)
View(geo_tweet_df)
geo_tweet_loc <- subset(geo_tweet_df, lat!="NA")
View(geo_tweet_loc)
geo_tweet_loc <- geo_tweet_loc[c("screen_name", "created_at", "text", "lat", "lng")]
geo_tweet_loc <- geo_tweet_loc %>%
group_by(lat, lng) %>%
mutate(concat_text = paste0(text, collapse = "<br/> <br/>")) %>%
select(-text)
geo_tweet_loc <- unique(geo_tweet_loc)
View(geo_tweet_loc)
write.csv(geo_tweet_loc,"charlieputh.csv")
geo_tweet_loc$longitude = as.numeric(as.character(geo_tweet_loc$lng))
geo_tweet_loc$latitude = as.numeric(as.character(geo_tweet_loc$lat))
map <- leaflet() %>%
addTiles() %>%
fitBounds(long1, lat1, long2, lat2) %>%
addMarkers(geo_tweet_loc$longitude, geo_tweet_loc$latitude, popup=geo_tweet_loc$concat_text)
map
pkgs <- c('vosonSML', 'rtweet', 'stringr', 'tm', 'SnowballC', 'ggplot2', 'igraph',
'slam', 'Rmpfr', 'rjson', 'leaflet', 'leaflet', 'httpuv', 'devtools', 'dplyr')
lapply(pkgs, library, character.only = TRUE)
View(trends_available())
current_trends <- get_trends(23424748)
View(current_trends)
lat1 <- 47.608013
long1 <- -122.335167
lat2 <- 29.424349
long2 <- -98.491142
map <- leaflet() %>%
addTiles() %>%
fitBounds(long1, lat1, long2, lat2)
map
geo_tweet <- search_tweets("charlieputh", lang="en", n=1000)
geo_tweet_df <- lat_lng(geo_tweet)
View(geo_tweet)
geo_tweet_loc <- subset(geo_tweet_df, lat!="NA")
View(geo_tweet_loc)
load("E:/Griffith University/4030BigData/milestone3_charlieputh/.RData")
`2019-11-26_142135-TwitterData` <- readRDS("E:/Griffith University/4030BigData/milestone3_charlieputh/2019-11-26_142135-TwitterData.rds")
pkgs <- c('igraph', 'magrittr', 'tuber', 'Rspotify', 'vosonSML')
lapply(pkgs, library, character.only = TRUE)
apiKey <- 'AIzaSyC7X31Uj8_xPnE8whKL-MoPMVqg14ap_Jg'
appID <- '597183295495-770rn2a1n9i7f3nt4kdtkbt2i47ivr96.apps.googleusercontent.com'
appSecret <- 'C2jeBjDQKZ5CChavf4LFSiC6'
yt_oauth(appID, appSecret)
searchResult <- yt_search("charlie puth")
View(searchResult)
# Create Actor nework
# Get first 10 video from the search results
#####   2.1) YouTube views/likes
videoIDs <- as.vector(searchResult$video_id[1:10])
videoState = lapply(videoIDs, function(x){
get_stats(video_id = x)
})
videoState = do.call(rbind.data.frame, videoState)
g_youtube_actor <- Authenticate("youtube", apiKey= apiKey) %>%
Collect(videoIDs = videoIDs, writeToFile=TRUE, verbose=TRUE, maxComments = 500)
View(g_youtube_actor)
youtubeActor <- g_youtube_actor %>% Create("actor")
# Save data in RDS format and load it
saveRDS(g_youtube_actor, file="youtube.rds")
g_youtube_actor_loaded = readRDS("youtube.rds")
undirected_youtubeActor <- as.undirected(youtubeActor$graph, mode="collapse")
imc_youtubeActor <- infomap.community(undirected_youtubeActor, nb.trials = 10)
# Summarize User distribution
communityMembership_youtubeActor <- membership(imc_youtubeActor)
communityDistribution_youtubeActor <- summary(as.factor(communityMembership_youtubeActor))
communityDistribution_youtubeActor
communities_youtubeActor <- communities(imc_youtubeActor)
length(communities_youtubeActor)
tail(sort(communityDistribution_youtubeActor), n=5)
communities_youtubeActor[names(tail(sort(
communityDistribution_youtubeActor), n=5))]
youtubeActor_Louvain <- cluster_louvain(undirected_youtubeActor, weights = NA)
communities(youtubeActor_Louvain)
sizes(youtubeActor_Louvain)
#Louvain_table <- data.frame(youtubeActor_Louvain$membership, youtubeActor_Louvain$names)
#head(Louvain_table)
plot(youtubeActor_Louvain, undirected_youtubeActor,
vertex.label = V(undirected_youtubeActor)$name,
vertex.size=4, vertex.label.cex=0.7)
write.graph(undirected_youtubeActor,"undirected_youtubeActorcp.graphml",format="graphml")
youtubeActor_EdgeBetween <- cluster_edge_betweenness(undirected_youtubeActor)
communities(youtubeActor_EdgeBetween)
sizes(youtubeActor_EdgeBetween)
EdgeBetween_table <- data.frame(youtubeActor_EdgeBetween$membership, youtubeActor_EdgeBetween$names)
head(EdgeBetween_table)
plot(youtubeActor_EdgeBetween, undirected_youtubeActor,
vertex.label = V(undirected_youtubeActor)$name,
vertex.size=4, vertex.label.cex=0.7)
write.graph(undirected_youtubeActor,"undirected_youtubeActor.graphml",format="graphml")
is_hierarchical(youtubeActor_EdgeBetween)
as.dendrogram(youtubeActor_EdgeBetween)
plot_dendrogram(youtubeActor_EdgeBetween,mode="dendrogram",xlim=c(1,20))
#plot_dendrogram(youtubeActor_EdgeBetween)
